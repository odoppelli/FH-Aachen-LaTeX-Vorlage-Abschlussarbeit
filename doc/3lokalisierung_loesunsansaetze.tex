\chapter{\textbf{Lokalisierung - Lösungsansätze}}\label{chap:Lokalisierung - Lösungsansätze}
\addtocontents{toc}{\vspace{0.8cm}}
Das Themengebiet der Lokalisierung in der mobilen Robotik ist ein Grundstein für die Funktionalität dieser. Ohne Informationen über den genauen Standort könnte ein Räsenmäherroboter den Garten nicht schön halten, ein nach einem Paket greifenden UAV das Ziel nicht treffen oder eine Handvoll Fußball spielender Roboter den Ball nicht treffen. Dabei sind drei verschiedene Arten der Lokalisierung ausschlaggebend: Die \textit{Global Localization}, das \textit{Position Tracking} und die \textit{Kidnapping Recovery}. Die globale Lokalisierung beschreibt die Fähigkeit des Systems die korrekte erste Position in einer gegebenen Karte zu bestimmen. Das Position Tracking befasst sich mit der Aufgabe eine gefundene Position bei Bewegung des Systems beizubehalten, während die Kidnapping Recovery die Herausforderung beschreibt den Standort des mobilen Roboters nach einem Entführungsszenario wiederherzustellen. Ein Entführungsszenario meint hierbei das Bewegen des Systems, ohne dass die externen, Umgebung erfassenden Sensoren aktiv sind. In den folgenden Kapiteln werden ein paar Lösungsansätze und ausformulierte Algorithmen zur Lösung des Lokalisierungsproblems vorgestellt.

\section{Monte Carlo Localization}\label{sec:mcl}
\addtocontents{toc}{\vspace{0.8cm}}
Der von Daeller et al. \cite{Monte-Carlo-Localization} entwickelte \textit{Monte Carlo Localization} (MCL) Lokalisierungsalgorithmus verfolgt das Ziel einen mobilen Roboter in bekannter Umgebung zu orten. Der Algorithmus bekommt eine 2D Occupancy Grid Map, das Motion-Model und das Oberservation-Model des Systems zur Verfügung gestellt, generiert damit eine geschätzte 2D Position und ist in der Lage sowohl Position-Tracking, Globale Lokalisierung, als auch Kidnapping-Recovery zu absolvieren. \\
Es handelt sich hierbei um einen Partikelfilter, bei dem jedes Partikel eine mögliche Position des mobilen Systems ohne Toleranz darstellt. Die Einschätzung der Position $Bel(l)$ wird dargestellt durch ein \textit{N}-zahliges Set von gewichteten, zufälligen Partikeln oder \textit{Samples}.Ein Sample kann dargestellt werden durch $s=\langle\langle x,y,\psi \rangle,p\rangle$, wobei die $\langle x,y,\psi$ die Position, und $p$ das Gewicht beziehungsweise die Wahrscheinlichkeit dieser Position repräsentiert. Das Vorgehen ist eingeteilt in die Schritte \textbf{Robot Motion} und \textbf{Sensor readings}. Folglich wird der Algorithmus beginnend mit dem Schritt \textbf{Sensor readings} erläutert.\\
Im Beispiel \ref{fig:mcl} wird die eindimensionale Position eines mobiler Roboter berechnet, indem er auf einer Geraden an mehreren Türen entlang fährt. Ein außen montierter Sensor gibt an, ob er sich an einer Tür befindet. Zum Zeitpunkt der Intialisierung des Algorithmus wird das erste Set an Samples mit vollkommen willkürlichen Positionen, und dem Gewicht $p=N^{-1}$ generiert. Dann folgt der Schritt Sensor readings \ref{fig:s_mcl_sensor} wird das Observation-Model genutzt um die Umgebung wahrzunehmen, und von jedem individuellen Sample aus zu berechnen wie wahrscheinlich die erhaltene Messung an der Position \textit{l} des Samples ist. Die Gewichtungen $p$ der einzelnen Samples in dem Sample-Set werden dann entsprechend der Ergebnisse angepasst, sodass $\sum_{n=1}^{N} p_n =1$ gilt. Je wahrscheinlicher die reale Messung am Ort des Samples ist, desto höher ist dessen Gewichtung.\\
Es folgt der Schritt Robot Motion \ref{fig:s_mcl_motion}: Der Roboter bewegt sich, der Algorithmus generiert ein neues Set mit $N$ neuen Samples, die mithilfe des Motion-Models die neue Position nach der Bewegung annähern. Dafür werden per Zufallsverfahren Samples aus dem vorherigen Set entnommen gegebenenfalls vervielfacht und jedes Gewicht durch $N^{-1}$ ersetzt. Zusätzlich werden dem Set ein wenig neue Samples mit willkürlich generierten Positionen hinzugefügt. Das hilft dem System dabei sich im Falle eines Kidnapping-Szenarios neu zu lokalisieren. Anschließend macht der mobile Roboter wieder Gebrauch von seinem Observation-Model und führt den Schritt Sensor readings wieder aus. Das Partikel mit dem zuletzt höchsten Gewicht stellt die aktuelle Position des Roboters dar. Je mehr Iterationen durchgeführt werden, desto näher befinden sich die Partikel beieinander und desto genauer wird die angenäherte Position.
\mbox{}
\begin{figure}
  \begin{subfigure}[t]{.5\textwidth}
    \centering
    \includegraphics[width=.9\linewidth]{pic/loesungen/1a_mcl.png}
    \caption{Sensor readings}
    \label{fig:s_mcl_sensor}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.5\textwidth}
    \centering
    \includegraphics[width=.9\linewidth]{pic/loesungen/1b_mcl.png}
    \caption{Robot Motion}
    \label{fig:s_mcl_motion}
  \end{subfigure}
  \caption{Der MCL Algorithmus aufgeteilt in \textbf{Sensor readings} und \textbf{Robot Motion}}
  \label{fig:mcl}
\end{figure}
\mbox{}


\subsection{Adaptive Monte Carlo Localization}\label{subsec:amcl}
\addtocontents{toc}{\vspace{0.8cm}}
Der Adaptive Monte Carlo Localization (AMCL) Algorithmus ist eine leichte Abwandlung gegenüber des herkömmlichen MCL Algorithmus, der die Größe des Sample-Sets variiert. In MCL ist die Größe des Sample-Sets nach einmaliger Einstellung unveränderbar, dh. es werden für die Berechnung der Position in jeder Iteration die gleiche Menge an Partikeln benutzt. AMCL verfolgt das Ziel einer zumindest gleichwertigen, wenn nicht sogar besseren Positionsbestimmung wie MCL, will dabei aber ressourcenschonender sein. Das erreicht der Algorithmus, indem er die Anzahl der genutzten Partikel abhängig von der Unsicherheit über die errechnete Position dynamisch anpasst.\\
Das Position Tracking benötigt für eine genügend genaue Standortbestimmung wesentlich weniger zu berechnende Partikel, als die weitaus größere Herausforderung der globalen Lokalisierung oder der Kidnapping Recovery. Für die dynamische Festlegung der Größe der Sets wird die Abweichung zwischen dem \textit{belief} vor und nach der Messungen der externen Sensoren berechnet. Je größer hierbei die Überraschung der Sensordaten ist, desto größer wird das nächste Sample-Set gewählt. Der AMCL Algorithmus wird auch heutzutage, trotz seines Alters, in vielen Applikationen verwendet, da es schnell eine Position mit geringem Fehler berechnet.
% TODO Behauptungen hier belegen


\subsection{Monte Carlo Lokalisierung mit Surfel Grid Maps}\label{subsec:mcl_surfel}
\addtocontents{toc}{\vspace{0.8cm}}
\begin{wrapfigure}{R}{0.3\textwidth}
    \centering
    \includegraphics[width=0.28\textwidth]{pic/loesungen/12_surfel.png}
    \caption{Surface Element in einem Voxel}
    \label{fig:surfel}
\end{wrapfigure}
Kläß et al. \cite{3DsurfelGridMaps} haben 2012 einen Algorithmus vorgestellt, dessen Ziel eine 2D Positionsbestimmung anhand einer 3D Voxel Grid Map ist. Die Lokalisierung wurde in der Veröffentlichung indoor durchgeführt und soll auch in dynamischen Szenarien, wenn sich beispielsweise bewegende Menschen in der Umgebung befinden, genau funktionieren können.\\
Die Besonderheit dieser Herangehensweise liegt in der Repräsentation der Umgebung. Die Umgebung wird durch eine vielzahl an Voxel (Würfel) modelliert, in denen sich jeweils ein \textit{Surface Element} oder kurz Surfel befindet (siehe Bild \ref{fig:surfel}). Befindet sich bei der Modellierung der Umgebung ein Objekt in einem Voxel, so wird ein Surfel entsprechend der Oberfläche des Objektes im Voxel generiert.\\
Die dahinter liegenden Berechnungen basieren auf MCL. Der Schritt Robot Motion bleibt bestehen, der der Sensor readings wird allerdings den neuen Techniken abgewandelt. Die Autoren schlagen für die verwendeten Sensor Models je nach Bedarf das \textit{Plane-to-Plane Model} oder das \textit{Line-to-Plane Model} vor. Ersteres stellt ein Oberservation-Model speziell für Sensorik, die 3D Scans in der 3D Surfel Grid Map durchführen soll, wie etwa LiDAR vor. Das Line-to-Plane Model hingegen zeigt eine mögliche Implementierung für Sensorik, die 2D Scans in der 3D Surfel Grid Map leisten soll. Aufgrund der fehlenden Dimension kann in letzterem Modell die Orientierung der Normale des Surfels nicht berücksichtigt werden.\\
Das System wurde vor allem in Hinblick auf die Navigation von bodengebundenen mobilen Robotern entwickelt. Das neuartige Surface Element soll dazu dienen die problematischen Diskretisierungseffekete von herkömmlichen 3D Voxel Grid Maps zu verringern (siehe Bild \ref{fig:discretization_effects}). Außerdem soll die Lokalisierung in dynamischen Szenarien erleichtert werden, indem der Algorithmus sich die 3D Map vollständig zu Nutze macht und sich dann auf Messungen über der Höhe des Menschen konzentriert um eine erfolgreiche Ortung durchzuführen.
% TODO Ergebnisse der Experimente einbringen
\mbox{}
\begin{figure}[ht]
  \begin{subfigure}[t]{.5\textwidth}
    \centering
    \includegraphics[width=.9\linewidth]{pic/loesungen/12a_steps.png}
    \caption{Generierung von Stufen}
    \label{fig:s_12steps}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{.5\textwidth}
    \centering
    \includegraphics[width=.9\linewidth]{pic/loesungen/12b_bumps.png}
    \caption{}
    \label{fig:s_12bumps}
  \end{subfigure}
  \caption{typische Diskretisierungseffekte in Occupancy Voxel Maps \cite{3DsurfelGridMaps}}
  \label{fig:discretization_effects}
\end{figure}
\mbox{}

\newpage
\subsection{Dreidimensionale Indoor Lokalisierung}\label{subsec:3d_indoor}
\addtocontents{toc}{\vspace{0.8cm}}
\begin{wrapfigure}{R}{0.5\textwidth}
    \centering
    \includegraphics[width=0.48\textwidth]{pic/loesungen/13_system.png}
    \caption{Systemaufbau}
    \label{fig:13_system}
\end{wrapfigure}
Perez-Grau et al. \cite{dreidimensionale_indoor_lokalisierung} haben 2017 ein System zur dreidimensionalen Positionsbestimmung eines UAV in einem geschlossenen Raum vorgestellt. Der Ansporn zu dessen Entwurf liegt in der industriellen Nutzung von UAVs, wie beispielsweise dem Transportieren von Gegenständen.\\
Die Umgebung wird in Voxeln modelliert und es werden mehrere Sensoren benutzt um mithilfe des MCL Algorithmus eine 3D Position errechnen zu können (siehe Bild \ref{fig:13_system}). Eine RGB-D Kamera wird genutzt um sowohl RGB-Bilder, als auch Point Clouds erfassen zu können. Außerdem werden drei sogenannte \textit{ultra-wideband Beacons} (UWB Beacons) genutzt, um per Triangulation einen Standort des UAVs im Raum errechnen zu können. Die Odometry ist hierbei nicht nur von einer IMU abhängig, sondern nutzt ebenfalls die Bilder der Kamera, als auch dessen Point Clouds. So soll eine möglichst genau \textit{Prediction} (Vorhersage) im Robot Motion Schritt des MCL generiert werden, die dann durch ein Update (Sensor readings) berichtigt wird. Diese Berichtigung ist abhängig von den 3D Point Clouds der Kamera, der 3D Voxel Map und den Distanzen zu den drei, im Raum verteilten, UWB Beacons. Die daraus errechnete 3D Position beinhaltet nicht die Nick- und Rollwinkel. Diese werden direkt aus den Berechnungen der IMU entnommen.\\
Der Raum in dem das System getestet wurde ist mit Kameras ausgestattet worden, um den wahren Standort des UAVs, unabhängig vom entworfenen System, erhalten zu können. So konnte ein mehr als 500 Sekunden langer Testflug unternommen werden, bei dem der Fehler der Odometry und der Fehler des MCL genau bestimmt werden konnten (siehe Bild \ref{fig:13_testergebnisse}).
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{pic/loesungen/13_testergebnisse.png}
    \caption{Testergebnisse der Autoren: links ist die Abweichung der Odometry, rechts die Abweichung nach Durchlauf des MCL angegeben \cite{dreidimensionale_indoor_lokalisierung}}
    \label{fig:13_testergebnisse}
\end{figure}


\section{Leader-Based Bat Algorithm}\label{sec:lbba}
\addtocontents{toc}{\vspace{0.8cm}}
Ähnlich wie die MCL, basiert der \textit{Leader-Based Bat Algorithm}(LBBA) ebenfalls auf Partikeln. Diese stellen in dem von Neto et al.\cite{LBBA} vorgeschlagenen Modell Fledermäuse dar, die ihre Inspiration in den \textit{Microbats} der realen Welt finden. Diese Fledermäuse fressen Insekten, und müssen ihre Umwelt und ihre Beute dazu präzise wahrnehmen können. Dazu fliegen sie an einem Ort \textit{x} mit einer Geschwindigkeit \textit{v} und nutzen die sogenannte Echolokation zur Wahrnehmung ihrer Umgebung. Dazu geben sie Ultraschallwellen von sich, nehmen die reflektierenden Wellen durch ihre Ohren wahr und bilden sich so ein Bild von ihrer Umwelt.\\
Yang et al.\cite{BA} implementierten und imitierten dieses Verhalten in dem \textit{Bat-Algorithm} (BA), dessen Aufgabenbereich in der globalen Optimierung liegt. Neto et al.\cite{LBBA} ergänzten diesen Algorithmus, mit dem Ziel ihn auf das Lokalisierungsproblem von mobilen Robotern anzupassen und darauf hin zu optimieren. Im Algorithmus werden $N$ viele Fledermäuse initialisiert, die eine zufällig zugeteilte Position $x$ und Geschwindigkeit $v$ haben. Außerdem wird die minimale- und maximale Frequenz,$F_{min}$ und $F_{max}$  (typischerweise [0,1]), die Lautstärke $A$ und die \textit{pulse emission rate} initialisiert. Der Ablauf des Algorithmus ist in Bild \ref{fig:lbba_code} dargestellt. Das Gewicht der Fledermäuse, bzw ihre \textit{fitness-function},, wird durch die Abweichung der am Ort der Fledermaus simulierten Sensor-Messung und der wahren Messung errechnet. Dies kann beispielsweise mithilfe des Observation-Models oder der Formel $f(x_i) = \frac{m_i -m}{m}$ erfolgen, wobei $m_i$ die simulierte Messung und m die wahre Messung darstellt.\\
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{pic/loesungen/20_lbba_algorithm.png}
    \caption{Der LBBA Code\cite{LBBA}. Die Variablen $\xi_{1-3}$ stellen zufällige Größen dar.}
    \label{fig:lbba_code}
\end{figure}
Der LBBA unterscheidet sich vom herkömmlichen BA in der Anzahl der besten Fledermäuse. Im BA wird bei jede Iteration genau eine Fledermaus zur besten Fledermaus in Hinsicht auf ihre fitness-Function gekürt. Dieser Alle anderen Fledermäuse bewegen sich in den darauf folgenden Iterationen auf diese Fledermaus zu, wobei eine Möglichkeit besteht, dass sie eine bessere fitness-Function als die ehemals beste Fledermaus haben, womit sie wiederum zur besten Fledermaus gekürt werden. Die Schleife wird so lange durchlaufen, bis ein Endkriterium erfüll wird. Im LBBA gibt es eine global beste Fledermaus, und eine benutzerdefinierte Anzahl an besten Fledermäusen. Alle Fledermäuse in einer bestimmten Nähe zur nächsten besten Fledermaus richten sich nach dieser aus und folgen dieser. Dies führt dazu, dass mehr Möglichkeiten der Lösung berücksichtigt werden, als im ursprünglichen BA.\\
Der LBBA ist im Kern ein offline-Algorithmus. Ihm genügt bereits eine Messung der externen Sensoren, um einen Standort errechnen zu können, was in der Aufgabe der globalen Lokalisierung von starkem Nutzen ist. Er kann allerdings auch online - also beispielsweise zum Position Tracking - eingesetzt werden. Die Autoren haben den LBBA unter anderem mit der AMCL und dem BA verglichen. Dabei haben sie experimentell festgestellt, dass LBBA mit nur wenigen Iterationen der \textit{while-Schleife} die präzisesten Ergebnisse liefert. Allerdings wird darauf hingedeutet, dass AMCL um ein vielfaches schneller ist sobald die globale Position einmal gefunden wurde.


\section{Präzise Vehikel Lokalisierung im Urbanen Bereich}\label{sec:vehicle_urban}
\addtocontents{toc}{\vspace{0.8cm}}
Chong et al. \cite{vehicleLocalizationUrban} haben ein Lokalisierungssystem entwickelt, welches eine 2D Position eines Vehikels, im speziellen eines Golf-Carts, in einer urbanen Umgebung ermittelt. Das System verfolgt das Ziel einer präzisen Lokalisierung mit nur einem angewinkelten LIDAR und der Odometry. Es vermeidet damit die Nutzung des GPS (Global Positioning System), welches im urbanen Bereich nur bedingt zuverlässig funktioniert, da höhere Gebäude zur Blockung von Satelliten oder ähnlichen Fehlern führen können. Außerdem nutzt es eine vorher erschaffene Karte und kleinere LIDAR Sensoren zur Hinderniss-Erkennung.\\
Das System, dargestellt in Bild \ref{fig:30_system}, nutzt die, durch den nach unten angewinkelten 3D LIDAR Sensor, aufgenommen Point Clouds, um vertikale Objekte zu erkennen. Dazu wird eine 3D Ebene an die Point Clouds per \textit{Least-Squares Fitting} angepasst. Dann wird die Normale der Ebene genutzt, um dazu passende Vertikale Objekte, mithilfe optimierter Klassifikation der Punkte, zu finden. Die gefundenen Features werden dann auf einen synthetischen 2D LIDAR Sensor horizontal projiziert, indem die z-Kooridnate auf Null gesetzt wird. Der virtuelle Sensor wird dann auf die Base des Fahrzeugs gesetzt, um zusammen mit der Odometry, den extrahierten vertikalen Features und der vorher erstellten Karte in einem AMCL Algorithmus die Ermittlung der Position des Golf-Carts zu ermöglichen.
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.8\textwidth]{pic/loesungen/30_System.png}
    \caption{Das vorgeschlagene System \cite{vehicleLocalizationUrban}}
    \label{fig:30_system}
\end{figure}
Durch die Projizierung der 3D LIDAR Daten auf einen virtuellen 2D LIDAR Sensor, ist es dem virtuellen Sensor möglich "durch Objekte hindurch schauen zu können". Das führt, geschickt implementiert, zur Detektierung von mehr Features, als es mit einem 2D LIDAR Scanner möglich gewesen wäre.\\
Dem Algorithmus wurde eine ungefähre Start-Position gegeben, und im Falle eines größeren Kidnapping-Szenarios wurde ihm ebenfalls eine ungefähre Position mitgeteilt. So hatte der AMCL Algorithmus fast nur die Aufgabe des Position Tracking. Die Autoren geben an, dass bei einer längeren Fahrt in der \textit{NUS engineering area} (also auf dem Campus der National University of Singapur) die maximale Varianz bei 0.2 Metern lag.
